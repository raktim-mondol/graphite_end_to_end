# XAI Visualization Tool for Histopathology Images

A comprehensive toolkit for explaining AI model predictions on histopathology images using various explainability methods.

## ğŸ”¬ Overview

This tool provides multiple explainability methods for analyzing histopathology images:

### 1. CAM-based Methods
- **GradCAM**: Gradient-weighted Class Activation Mapping
- **HiResCAM**: High Resolution Class Activation Mapping  
- **ScoreCAM**: Score-weighted Class Activation Mapping
- **GradCAM++**: Improved version of GradCAM
- **AblationCAM**: Ablation-based CAM
- **XGradCAM**: Extended GradCAM
- **EigenCAM**: Eigen-based CAM
- **FullGrad**: Full Gradient decomposition

### 2. Model-agnostic Methods
- **SHAP Deep Explainer**: Deep learning-specific SHAP explanations
- **SHAP Gradient Explainer**: Gradient-based SHAP explanations
- **LIME**: Local Interpretable Model-agnostic Explanations

### 3. MIL Attention-based
- **Attention Visualization**: Multiple Instance Learning attention weights

## ğŸš€ Quick Start

### Setup

**Note:** For initial project setup and dependencies, please refer to the main project README.md in the root directory.

This module requires specific XAI dependencies. Ensure you have the following installed:

```bash
# Additional XAI-specific requirements
pip install grad-cam
pip install shap
pip install lime
pip install grad-cam
```

### Basic Usage

```bash
# GradCAM visualization
python main.py --method gradcam \
    --wsi_folder ./data/wsi \
    --mask_folder ./data/masks \
    --output_folder ./results \
    --model_path ./models/best_model.pth

# SHAP visualization
python main.py --method shap_deep \
    --wsi_folder ./data/wsi \
    --mask_folder ./data/masks \
    --output_folder ./results

# LIME visualization
python main.py --method lime \
    --wsi_folder ./data/wsi \
    --mask_folder ./data/masks \
    --output_folder ./results

# Attention visualization
python main.py --method attention \
    --wsi_folder ./data/wsi \
    --mask_folder ./data/masks \
    --output_folder ./results
```

## ğŸ“ Project Structure

```
xai_visualization/
â”œâ”€â”€ main.py                    # Main CLI interface
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ config/
â”‚   â””â”€â”€ default.yaml          # Default configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ slide_reader.py   # WSI reading utilities
â”‚   â”‚   â””â”€â”€ color_normalizer.py # Color normalization
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ mil_model.py      # MIL model definition
â”‚   â”‚   â””â”€â”€ model_loader.py   # Model loading utilities
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py         # Configuration management
â”‚   â”‚   â”œâ”€â”€ logger.py         # Logging utilities
â”‚   â”‚   â””â”€â”€ metrics.py        # Evaluation metrics
â”‚   â””â”€â”€ visualizers/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_visualizer.py    # Base class for all visualizers
â”‚       â”œâ”€â”€ cam_visualizer.py     # CAM-based methods
â”‚       â”œâ”€â”€ shap_visualizer.py    # SHAP-based methods
â”‚       â”œâ”€â”€ lime_visualizer.py    # LIME-based methods
â”‚       â””â”€â”€ attention_visualizer.py # Attention-based methods
â”œâ”€â”€ examples/
â”œâ”€â”€ docs/
â””â”€â”€ tests/
```

## ğŸ› ï¸ Configuration

The tool uses YAML configuration files. You can customize parameters in `config/default.yaml`:

```yaml
# Model parameters
feat_dim: 512
proj_dim: 128
num_classes: 2

# Image processing
patch_size: 224
stride: 224
target_class: 1

# Method-specific parameters
background_samples: 10  # For SHAP
lime_num_samples: 100   # For LIME
```

## ğŸ“Š Available Methods

### CAM-based Methods
All CAM methods support the following parameters:
- `--patch_size`: Size of patches (default: 224)
- `--stride`: Stride between patches (default: 224)
- `--target_class`: Target class for visualization (default: 1)

### SHAP Methods
- `shap_deep`: Uses DeepExplainer for neural networks
- `shap_gradient`: Uses GradientExplainer for gradient-based explanations

### LIME
- Provides local explanations using image segmentation
- Configurable number of samples and features

### Attention
- Visualizes attention weights from MIL models
- Requires models with attention mechanisms

## ğŸ“ˆ Output

The tool generates:
1. **Heatmap visualizations**: PNG files showing explanations overlaid on WSI
2. **Evaluation metrics**: CSV files with quantitative results
3. **Summary statistics**: Overall performance metrics

### Metrics Calculated
- Precision
- Recall  
- F1-score
- IoU (Intersection over Union)
- Confusion matrix elements (TP, FP, FN, TN)

## ğŸ”§ Advanced Usage

### Custom Configuration
```bash
python main.py --method gradcam \
    --config ./config/custom.yaml \
    --wsi_folder ./data/wsi \
    --mask_folder ./data/masks \
    --output_folder ./results
```

### Batch Processing
The tool automatically processes all PNG files in the WSI folder and matches them with corresponding masks.

### GPU Support
```bash
python main.py --method gradcam \
    --device cuda \
    --wsi_folder ./data/wsi \
    --mask_folder ./data/masks \
    --output_folder ./results
```

## ğŸ“‹ Requirements

- Python 3.9.2 or higher
- PyTorch 2.0.0
- CUDA 11.7
- NVIDIA Tesla V100 32GB (used/recommended) or equivalent CUDA-compatible GPU (optional, for GPU acceleration)

See `requirements.txt` for complete dependency list.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

